{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mind2Web Multimodal Baselines\n",
    "## Baselines: Text DOM, Image, Multimodal, AXTree, CoT variants\n",
    "## Models: Qwen2-VL-7B, InternVL2-8B\n",
    "\n",
    "**Prerequisites:** Your teammate's dataloader should expose a `Mind2WebDataset` class.  \n",
    "This notebook handles model loading, prompting, inference, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, AutoModel, AutoTokenizer\n",
    "\n",
    "# W&B (optional)\n",
    "# import wandb\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION — edit these paths\n",
    "# ============================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    \"data_root\": \"/home/ubuntu/mind2web_data\",       # your teammate's dataset path\n",
    "    \"results_dir\": \"/home/ubuntu/results\",\n",
    "    \"hf_cache\": \"/home/ubuntu/hf_cache\",\n",
    "\n",
    "    # Model IDs\n",
    "    \"model_a\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    \"model_b\": \"OpenGVLab/InternVL2-8B\",\n",
    "\n",
    "    # Inference\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"dtype\": torch.bfloat16,\n",
    "    \"max_new_tokens\": 256,          # increase to 512+ for CoT baselines\n",
    "    \"batch_size\": 1,                # keep at 1 for simplicity; increase if VRAM allows\n",
    "\n",
    "    # Experiment\n",
    "    \"max_samples\": None,            # set to e.g. 100 for quick debugging; None = full dataset\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "os.environ[\"HF_HOME\"] = CONFIG[\"hf_cache\"]\n",
    "Path(CONFIG[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Config loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Interface\n",
    "Replace the stub below with your teammate's actual dataloader import once ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "class Mind2WebDataset:\n",
    "    \"\"\"\n",
    "    Real loader for osunlp/Multimodal-Mind2Web.\n",
    "    Available splits: 'train', 'test_task', 'test_website', 'test_domain'\n",
    "    \"\"\"\n",
    "    def __init__(self, split=\"test_website\", streaming=True):\n",
    "        # We use streaming=True to pull data on-the-fly from AWS/HF\n",
    "        self.ds = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=split, streaming=streaming)\n",
    "        self.iter_ds = iter(self.ds)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for row in self.ds:\n",
    "            yield self._process_row(row)\n",
    "\n",
    "    def _process_row(self, row):\n",
    "        \"\"\"Maps HuggingFace row schema to your project's expected interface.\"\"\"\n",
    "        # Note: Mind2Web provides 'cleaned_html'. \n",
    "        # For AXTree, you'll need a conversion step (see note below).\n",
    "        \n",
    "        return {\n",
    "            \"action_uid\": row.get(\"action_uid\"),\n",
    "            \"instruction\": row.get(\"confirmed_task\"),\n",
    "            \"dom_text\": row.get(\"cleaned_html\"),  \n",
    "            \"axtree_text\": row.get(\"axtree\", \"\"), # Check if your version has axtree; otherwise requires conversion\n",
    "            \"screenshot\": row.get(\"image\"),       # This is already a PIL Image object\n",
    "            \"candidate_elements\": row.get(\"pos_candidates\", []) + row.get(\"neg_candidates\", []),\n",
    "            \"label_element_id\": row.get(\"pos_candidates\")[0][\"backend_node_id\"] if row.get(\"pos_candidates\") else None,\n",
    "            \"label_action\": row.get(\"operation\", {}).get(\"op\"),\n",
    "            \"label_value\": row.get(\"operation\", {}).get(\"value\"),\n",
    "        }\n",
    "\n",
    "    def get_sample(self):\n",
    "        \"\"\"Helper to get just one sample for testing.\"\"\"\n",
    "        return self._process_row(next(self.iter_ds))\n",
    "\n",
    "# --- Usage in your Notebook ---\n",
    "# Since it's an iterable dataset (streaming), you loop through it or use next()\n",
    "dataset = Mind2WebDataset(split=\"test_website\", streaming=True)\n",
    "sample = dataset.get_sample()\n",
    "\n",
    "print(f\"Task: {sample['instruction']}\")\n",
    "print(f\"Action: {sample['label_action']} on ID {sample['label_element_id']}\")\n",
    "# display(sample['screenshot']) # Use this in Jupyter to see the image\n",
    "\n",
    "#dataset = Mind2WebDataset(split=\"test\")\n",
    "#print(f\"Dataset size: {len(dataset)} samples\")\n",
    "#print(\"Sample keys:\", list(dataset[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Templates\n",
    "One template per baseline type. Each returns a string prompt (and optionally an image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_candidates(candidate_elements):\n",
    "    \"\"\"Format candidate elements into a numbered list for the prompt.\"\"\"\n",
    "    lines = []\n",
    "    for i, el in enumerate(candidate_elements):\n",
    "        lines.append(f\"[{i}] id={el['element_id']} tag={el['tag']} text=\\\"{el['text']}\\\"\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a web agent. Given a task and webpage information, predict the next action.\\n\"\n",
    "    \"Respond ONLY in JSON format: {\\\"element_id\\\": \\\"<id>\\\", \\\"action\\\": \\\"<click|type|select>\\\", \\\"value\\\": \\\"<optional>\\\"}\\n\"\n",
    "    \"Do not include any explanation outside the JSON.\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_COT = (\n",
    "    \"You are a web agent. Given a task and webpage information, predict the next action.\\n\"\n",
    "    \"First, reason step by step about which element to interact with and why.\\n\"\n",
    "    \"Then respond with a JSON on the final line: {\\\"element_id\\\": \\\"<id>\\\", \\\"action\\\": \\\"<click|type|select>\\\", \\\"value\\\": \\\"<optional>\\\"}\"\n",
    ")\n",
    "\n",
    "\n",
    "def make_prompt(sample, baseline_type, use_cot=False):\n",
    "    \"\"\"\n",
    "    Returns: (text_prompt: str, image: PIL.Image or None)\n",
    "\n",
    "    baseline_type options:\n",
    "        'text_dom'       -> Baseline 1: DOM text only\n",
    "        'image_only'     -> Baseline 2: Image only\n",
    "        'multimodal_dom' -> Baselines 3/4: Image + DOM\n",
    "        'axtree_only'    -> Baseline 5: AXTree text only\n",
    "        'multimodal_ax'  -> Baselines 6/7: Image + AXTree\n",
    "    \"\"\"\n",
    "    instruction = sample[\"instruction\"]\n",
    "    candidates  = format_candidates(sample[\"candidate_elements\"])\n",
    "    dom_text    = sample[\"dom_text\"]\n",
    "    axtree_text = sample[\"axtree_text\"]\n",
    "    image       = sample[\"screenshot\"]\n",
    "\n",
    "    sys = SYSTEM_PROMPT_COT if use_cot else SYSTEM_PROMPT\n",
    "\n",
    "    if baseline_type == \"text_dom\":\n",
    "        prompt = (\n",
    "            f\"{sys}\\n\\n\"\n",
    "            f\"Task: {instruction}\\n\\n\"\n",
    "            f\"DOM:\\n{dom_text}\\n\\n\"\n",
    "            f\"Candidate elements:\\n{candidates}\\n\\n\"\n",
    "            f\"Next action:\"\n",
    "        )\n",
    "        return prompt, None\n",
    "\n",
    "    elif baseline_type == \"image_only\":\n",
    "        prompt = (\n",
    "            f\"{sys}\\n\\n\"\n",
    "            f\"Task: {instruction}\\n\\n\"\n",
    "            f\"Candidate elements:\\n{candidates}\\n\\n\"\n",
    "            \"The screenshot of the current webpage is provided.\\n\"\n",
    "            f\"Next action:\"\n",
    "        )\n",
    "        return prompt, image\n",
    "\n",
    "    elif baseline_type == \"multimodal_dom\":\n",
    "        prompt = (\n",
    "            f\"{sys}\\n\\n\"\n",
    "            f\"Task: {instruction}\\n\\n\"\n",
    "            f\"DOM:\\n{dom_text}\\n\\n\"\n",
    "            f\"Candidate elements:\\n{candidates}\\n\\n\"\n",
    "            \"The screenshot of the current webpage is provided.\\n\"\n",
    "            f\"Next action:\"\n",
    "        )\n",
    "        return prompt, image\n",
    "\n",
    "    elif baseline_type == \"axtree_only\":\n",
    "        prompt = (\n",
    "            f\"{sys}\\n\\n\"\n",
    "            f\"Task: {instruction}\\n\\n\"\n",
    "            f\"Accessibility Tree:\\n{axtree_text}\\n\\n\"\n",
    "            f\"Candidate elements:\\n{candidates}\\n\\n\"\n",
    "            f\"Next action:\"\n",
    "        )\n",
    "        return prompt, None\n",
    "\n",
    "    elif baseline_type == \"multimodal_ax\":\n",
    "        prompt = (\n",
    "            f\"{sys}\\n\\n\"\n",
    "            f\"Task: {instruction}\\n\\n\"\n",
    "            f\"Accessibility Tree:\\n{axtree_text}\\n\\n\"\n",
    "            f\"Candidate elements:\\n{candidates}\\n\\n\"\n",
    "            \"The screenshot of the current webpage is provided.\\n\"\n",
    "            f\"Next action:\"\n",
    "        )\n",
    "        return prompt, image\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown baseline_type: {baseline_type}\")\n",
    "\n",
    "\n",
    "print(\"Prompt templates defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Wrappers\n",
    "Unified `predict()` interface for both Qwen2-VL and InternVL2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2VLWrapper:\n",
    "    \"\"\"Wrapper for Qwen2-VL-7B-Instruct\"\"\"\n",
    "\n",
    "    def __init__(self, model_id=CONFIG[\"model_a\"]):\n",
    "        print(f\"Loading Qwen2-VL from {model_id}...\")\n",
    "        from qwen_vl_utils import process_vision_info\n",
    "        self.process_vision_info = process_vision_info\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=CONFIG[\"dtype\"],\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.model.eval()\n",
    "        print(\"Qwen2-VL loaded.\")\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, text_prompt, image=None, max_new_tokens=CONFIG[\"max_new_tokens\"]):\n",
    "        messages = []\n",
    "        content = []\n",
    "        if image is not None:\n",
    "            content.append({\"type\": \"image\", \"image\": image})\n",
    "        content.append({\"type\": \"text\", \"text\": text_prompt})\n",
    "        messages.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "        text = self.processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        image_inputs, video_inputs = self.process_vision_info(messages)\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(CONFIG[\"device\"])\n",
    "\n",
    "        generated = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        out = self.processor.decode(\n",
    "            generated[0][inputs.input_ids.shape[1]:], skip_special_tokens=True\n",
    "        )\n",
    "        return out.strip()\n",
    "\n",
    "\n",
    "class InternVL2Wrapper:\n",
    "    \"\"\"Wrapper for InternVL2-8B\"\"\"\n",
    "\n",
    "    def __init__(self, model_id=CONFIG[\"model_b\"]):\n",
    "        print(f\"Loading InternVL2 from {model_id}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=CONFIG[\"dtype\"],\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.model.eval()\n",
    "        print(\"InternVL2 loaded.\")\n",
    "\n",
    "    def _load_image(self, image, max_num=6):\n",
    "        \"\"\"Convert PIL image to InternVL2 pixel_values format.\"\"\"\n",
    "        import torchvision.transforms as T\n",
    "        from torchvision.transforms.functional import InterpolationMode\n",
    "        IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "        IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "        transform = T.Compose([\n",
    "            T.Resize((448, 448), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        ])\n",
    "        pixel_values = transform(image.convert(\"RGB\")).unsqueeze(0)\n",
    "        return pixel_values.to(CONFIG[\"dtype\"]).to(CONFIG[\"device\"])\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, text_prompt, image=None, max_new_tokens=CONFIG[\"max_new_tokens\"]):\n",
    "        gen_config = {\"max_new_tokens\": max_new_tokens, \"do_sample\": False}\n",
    "\n",
    "        if image is not None:\n",
    "            pixel_values = self._load_image(image)\n",
    "            prompt = f\"<image>\\n{text_prompt}\"\n",
    "            response = self.model.chat(\n",
    "                self.tokenizer, pixel_values, prompt, gen_config\n",
    "            )\n",
    "        else:\n",
    "            pixel_values = None\n",
    "            response = self.model.chat(\n",
    "                self.tokenizer, pixel_values, text_prompt, gen_config\n",
    "            )\n",
    "        return response.strip()\n",
    "\n",
    "\n",
    "print(\"Model wrappers defined. Run the next cell to load a model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output Parsing & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_prediction(raw_output):\n",
    "    \"\"\"\n",
    "    Extract JSON from model output.\n",
    "    Returns dict with keys: element_id, action, value\n",
    "    Returns None on parse failure.\n",
    "    \"\"\"\n",
    "    # Try to find JSON in the output (handles CoT where JSON is at the end)\n",
    "    json_pattern = r'\\{[^{}]*\\}'\n",
    "    matches = re.findall(json_pattern, raw_output, re.DOTALL)\n",
    "\n",
    "    for match in reversed(matches):  # take last JSON block (CoT reasoning first, JSON last)\n",
    "        try:\n",
    "            parsed = json.loads(match)\n",
    "            if \"element_id\" in parsed and \"action\" in parsed:\n",
    "                parsed.setdefault(\"value\", \"\")\n",
    "                return parsed\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def evaluate_prediction(pred, sample):\n",
    "    \"\"\"\n",
    "    Returns dict of per-sample metrics.\n",
    "    \"\"\"\n",
    "    if pred is None:\n",
    "        return {\"element_acc\": 0, \"action_acc\": 0, \"exact_match\": 0, \"parse_fail\": 1}\n",
    "\n",
    "    element_correct = (str(pred[\"element_id\"]) == str(sample[\"label_element_id\"]))\n",
    "    action_correct  = (pred[\"action\"].lower() == sample[\"label_action\"].lower())\n",
    "    exact_match     = element_correct and action_correct\n",
    "\n",
    "    return {\n",
    "        \"element_acc\": int(element_correct),\n",
    "        \"action_acc\":  int(action_correct),\n",
    "        \"exact_match\": int(exact_match),\n",
    "        \"parse_fail\":  0,\n",
    "    }\n",
    "\n",
    "\n",
    "def aggregate_metrics(results):\n",
    "    keys = [\"element_acc\", \"action_acc\", \"exact_match\", \"parse_fail\"]\n",
    "    agg = {k: np.mean([r[k] for r in results]) for k in keys}\n",
    "    agg[\"n\"] = len(results)\n",
    "    return agg\n",
    "\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Inference Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(\n",
    "    model_wrapper,\n",
    "    dataset,\n",
    "    baseline_type,\n",
    "    baseline_name,\n",
    "    use_cot=False,\n",
    "    max_samples=CONFIG[\"max_samples\"],\n",
    "    results_dir=CONFIG[\"results_dir\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Run inference for one baseline and save results.\n",
    "\n",
    "    Args:\n",
    "        model_wrapper : Qwen2VLWrapper or InternVL2Wrapper instance\n",
    "        dataset       : Mind2WebDataset\n",
    "        baseline_type : str — one of 'text_dom', 'image_only', 'multimodal_dom',\n",
    "                               'axtree_only', 'multimodal_ax'\n",
    "        baseline_name : str — used for output file naming (e.g. 'B1_text_dom_qwen')\n",
    "        use_cot       : bool — whether to use chain-of-thought prompt\n",
    "        max_samples   : int or None — limit for debugging\n",
    "    \"\"\"\n",
    "    max_new_tokens = 512 if use_cot else CONFIG[\"max_new_tokens\"]\n",
    "    n = min(len(dataset), max_samples) if max_samples else len(dataset)\n",
    "\n",
    "    all_results = []\n",
    "    raw_outputs = []\n",
    "\n",
    "    for i in tqdm(range(n), desc=baseline_name):\n",
    "        sample = dataset[i]\n",
    "        prompt, image = make_prompt(sample, baseline_type, use_cot=use_cot)\n",
    "\n",
    "        try:\n",
    "            raw = model_wrapper.predict(prompt, image=image, max_new_tokens=max_new_tokens)\n",
    "        except Exception as e:\n",
    "            print(f\"[{i}] Inference error: {e}\")\n",
    "            raw = \"\"\n",
    "\n",
    "        pred    = parse_prediction(raw)\n",
    "        metrics = evaluate_prediction(pred, sample)\n",
    "\n",
    "        all_results.append(metrics)\n",
    "        raw_outputs.append({\n",
    "            \"task_id\":   sample[\"task_id\"],\n",
    "            \"raw\":       raw,\n",
    "            \"parsed\":    pred,\n",
    "            \"gt_element\": sample[\"label_element_id\"],\n",
    "            \"gt_action\":  sample[\"label_action\"],\n",
    "            **metrics,\n",
    "        })\n",
    "\n",
    "    # Save raw outputs\n",
    "    out_path = Path(results_dir) / f\"{baseline_name}_raw.jsonl\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        for row in raw_outputs:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "    # Aggregate and save summary\n",
    "    summary = aggregate_metrics(all_results)\n",
    "    summary_path = Path(results_dir) / f\"{baseline_name}_summary.json\"\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        json.dump({\"baseline\": baseline_name, **summary}, f, indent=2)\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Baseline: {baseline_name}\")\n",
    "    print(f\"  Element Acc : {summary['element_acc']:.3f}\")\n",
    "    print(f\"  Action Acc  : {summary['action_acc']:.3f}\")\n",
    "    print(f\"  Exact Match : {summary['exact_match']:.3f}\")\n",
    "    print(f\"  Parse Fail  : {summary['parse_fail']:.3f}\")\n",
    "    print(f\"  N samples   : {summary['n']}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "print(\"run_baseline() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run All Baselines\n",
    "\n",
    "The 10 baselines map as follows:\n",
    "\n",
    "| # | Description | baseline_type | use_cot | model |\n",
    "|---|---|---|---|---|\n",
    "| 1 | Text DOM only | `text_dom` | False | either |\n",
    "| 2 | Image only | `image_only` | False | either |\n",
    "| 3 | Multimodal - Model A | `multimodal_dom` | False | Qwen2-VL |\n",
    "| 4 | Multimodal - Model B | `multimodal_dom` | False | InternVL2 |\n",
    "| 5 | AXTree only | `axtree_only` | False | either |\n",
    "| 6 | Multimodal + AXTree - Model A | `multimodal_ax` | False | Qwen2-VL |\n",
    "| 7 | Multimodal + AXTree - Model B | `multimodal_ax` | False | InternVL2 |\n",
    "| 8 | 3 + CoT | `multimodal_dom` | True | Qwen2-VL |\n",
    "| 9 | 5 + CoT - Model A | `multimodal_ax` | True | Qwen2-VL |\n",
    "| 10 | 5 + CoT - Model B | `multimodal_ax` | True | InternVL2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1: Load Model A (Qwen2-VL)\n",
    "# Run this cell, then run Qwen baselines below.\n",
    "# Then unload and load Model B to save VRAM.\n",
    "# ============================================================\n",
    "model_a = Qwen2VLWrapper()\n",
    "all_summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 1: Text DOM only (model-agnostic, run with model_a)\n",
    "all_summaries[\"B1_text_dom\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"text_dom\",\n",
    "    baseline_name=\"B1_text_dom_qwen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 2: Image only\n",
    "all_summaries[\"B2_image_only\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"image_only\",\n",
    "    baseline_name=\"B2_image_only_qwen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 3: Multimodal DOM - Model A (Qwen2-VL)\n",
    "all_summaries[\"B3_multimodal_dom_qwen\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"multimodal_dom\",\n",
    "    baseline_name=\"B3_multimodal_dom_qwen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 5: AXTree only\n",
    "all_summaries[\"B5_axtree_only\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"axtree_only\",\n",
    "    baseline_name=\"B5_axtree_only_qwen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 6: Multimodal AXTree - Model A (Qwen2-VL)\n",
    "all_summaries[\"B6_multimodal_ax_qwen\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"multimodal_ax\",\n",
    "    baseline_name=\"B6_multimodal_ax_qwen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 8: Multimodal DOM + CoT (Model A)\n",
    "all_summaries[\"B8_multimodal_dom_cot_qwen\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"multimodal_dom\",\n",
    "    baseline_name=\"B8_multimodal_dom_cot_qwen\",\n",
    "    use_cot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 9: AXTree + CoT - Model A (Qwen2-VL)\n",
    "all_summaries[\"B9_axtree_cot_qwen\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"multimodal_ax\",\n",
    "    baseline_name=\"B9_axtree_cot_qwen\",\n",
    "    use_cot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload Model A to free VRAM before loading Model B\n",
    "del model_a\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model A unloaded. VRAM freed.\")\n",
    "print(f\"VRAM used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: Load Model B (InternVL2)\n",
    "# ============================================================\n",
    "model_b = InternVL2Wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 4: Multimodal DOM - Model B (InternVL2)\n",
    "all_summaries[\"B4_multimodal_dom_internvl\"] = run_baseline(\n",
    "    model_b, dataset,\n",
    "    baseline_type=\"multimodal_dom\",\n",
    "    baseline_name=\"B4_multimodal_dom_internvl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 7: Multimodal AXTree - Model B (InternVL2)\n",
    "all_summaries[\"B7_multimodal_ax_internvl\"] = run_baseline(\n",
    "    model_b, dataset,\n",
    "    baseline_type=\"multimodal_ax\",\n",
    "    baseline_name=\"B7_multimodal_ax_internvl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 10: AXTree + CoT - Model B (InternVL2)\n",
    "all_summaries[\"B10_axtree_cot_internvl\"] = run_baseline(\n",
    "    model_b, dataset,\n",
    "    baseline_type=\"multimodal_ax\",\n",
    "    baseline_name=\"B10_axtree_cot_internvl\",\n",
    "    use_cot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "baseline_labels = {\n",
    "    \"B1_text_dom\":               \"B1: Text DOM only\",\n",
    "    \"B2_image_only\":             \"B2: Image only\",\n",
    "    \"B3_multimodal_dom_qwen\":    \"B3: Multimodal DOM (Qwen2-VL)\",\n",
    "    \"B4_multimodal_dom_internvl\":\"B4: Multimodal DOM (InternVL2)\",\n",
    "    \"B5_axtree_only\":            \"B5: AXTree only\",\n",
    "    \"B6_multimodal_ax_qwen\":     \"B6: Multimodal AXTree (Qwen2-VL)\",\n",
    "    \"B7_multimodal_ax_internvl\": \"B7: Multimodal AXTree (InternVL2)\",\n",
    "    \"B8_multimodal_dom_cot_qwen\":\"B8: Multimodal DOM + CoT (Qwen2-VL)\",\n",
    "    \"B9_axtree_cot_qwen\":        \"B9: AXTree + CoT (Qwen2-VL)\",\n",
    "    \"B10_axtree_cot_internvl\":   \"B10: AXTree + CoT (InternVL2)\",\n",
    "}\n",
    "\n",
    "for key, label in baseline_labels.items():\n",
    "    if key in all_summaries:\n",
    "        s = all_summaries[key]\n",
    "        rows.append({\n",
    "            \"Baseline\": label,\n",
    "            \"Element Acc\": f\"{s['element_acc']:.3f}\",\n",
    "            \"Action Acc\":  f\"{s['action_acc']:.3f}\",\n",
    "            \"Exact Match\": f\"{s['exact_match']:.3f}\",\n",
    "            \"Parse Fail\":  f\"{s['parse_fail']:.3f}\",\n",
    "            \"N\": int(s['n']),\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(Path(CONFIG[\"results_dir\"]) / \"all_baselines_summary.csv\", index=False)\n",
    "print(\"\\nSaved to all_baselines_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "x = range(len(df))\n",
    "ax.bar(x, df[\"Exact Match\"].astype(float), color=\"steelblue\", alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"B{i+1}\" for i in x], fontsize=9)\n",
    "ax.set_ylabel(\"Exact Match Accuracy\")\n",
    "ax.set_title(\"Mind2Web Baselines — Exact Match Accuracy\")\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    ax.text(i, float(row[\"Exact Match\"]) + 0.01, row[\"Exact Match\"], ha=\"center\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(CONFIG[\"results_dir\"]) / \"baseline_results.png\", dpi=150)\n",
    "plt.show()\n",
    "print(\"Plot saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Debugging Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a single sample and model output\n",
    "sample = dataset[0]\n",
    "print(\"Instruction:\", sample[\"instruction\"])\n",
    "print(\"Label element:\", sample[\"label_element_id\"])\n",
    "print(\"Label action:\", sample[\"label_action\"])\n",
    "\n",
    "prompt, image = make_prompt(sample, \"multimodal_ax\", use_cot=False)\n",
    "print(\"\\n--- Prompt (first 800 chars) ---\")\n",
    "print(prompt[:800])\n",
    "\n",
    "if image:\n",
    "    display(image.resize((400, 225)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect a saved results file\n",
    "results_file = Path(CONFIG[\"results_dir\"]) / \"B6_multimodal_ax_qwen_raw.jsonl\"\n",
    "if results_file.exists():\n",
    "    with open(results_file) as f:\n",
    "        lines = [json.loads(l) for l in f]\n",
    "    \n",
    "    failures = [l for l in lines if l[\"element_acc\"] == 0]\n",
    "    print(f\"Total: {len(lines)}, Failures: {len(failures)}\")\n",
    "    print(\"\\nExample failure:\")\n",
    "    print(json.dumps(failures[0], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
