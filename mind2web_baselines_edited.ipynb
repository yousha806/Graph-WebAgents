{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mind2Web Multimodal Baselines\n",
    "## Baselines: Text DOM, Image, Multimodal, AXTree, CoT variants\n",
    "## Models: Qwen2-VL-7B, InternVL2-8B\n",
    "\n",
    "**Prerequisites:** Your teammate's dataloader should expose a `Mind2WebDataset` class.  \n",
    "This notebook handles model loading, prompting, inference, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pytorch/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A10G\n",
      "VRAM: 23.7 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration, AutoModel, AutoTokenizer\n",
    "\n",
    "# W&B (optional)\n",
    "# import wandb\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION — edit these paths\n",
    "# ============================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    \"data_root\": \"/home/ubuntu/mind2web_data\",       # your teammate's dataset path\n",
    "    \"results_dir\": \"/home/ubuntu/results\",\n",
    "    \"hf_cache\": \"/home/ubuntu/hf_cache\",\n",
    "\n",
    "    # Model IDs\n",
    "    \"model_a\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    \"model_b\": \"OpenGVLab/InternVL2-8B\",\n",
    "\n",
    "    # Inference\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"dtype\": torch.bfloat16,\n",
    "    \"max_new_tokens\": 256,          # increase to 512+ for CoT baselines\n",
    "    \"batch_size\": 1,                # keep at 1 for simplicity; increase if VRAM allows\n",
    "\n",
    "    # Experiment\n",
    "    \"max_samples\": 100,            # set to e.g. 100 for quick debugging; None = full dataset\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "os.environ[\"HF_HOME\"] = CONFIG[\"hf_cache\"]\n",
    "Path(CONFIG[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Config loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Interface\n",
    "Replace the stub below with your teammate's actual dataloader import once ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24fdbb09c8d44629c617b1bf1d46e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5da647d3fc49e39e4c895881f44de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# --- Usage in your Notebook ---\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Since it's an iterable dataset (streaming), you loop through it or use next()\u001b[39;00m\n\u001b[32m     42\u001b[39m dataset = Mind2WebDataset(split=\u001b[33m\"\u001b[39m\u001b[33mtest_website\u001b[39m\u001b[33m\"\u001b[39m, streaming=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m sample = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[33m'\u001b[39m\u001b[33minstruction\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[33m'\u001b[39m\u001b[33mlabel_action\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[33m'\u001b[39m\u001b[33mlabel_element_id\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mMind2WebDataset.get_sample\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_sample\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Helper to get just one sample for testing.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_row\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_ds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mMind2WebDataset._process_row\u001b[39m\u001b[34m(self, row)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Maps HuggingFace row schema to your project's expected interface.\"\"\"\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Note: Mind2Web provides 'cleaned_html'. \u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# For AXTree, you'll need a conversion step (see note below).\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maction_uid\u001b[39m\u001b[33m\"\u001b[39m: row.get(\u001b[33m\"\u001b[39m\u001b[33maction_uid\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minstruction\u001b[39m\u001b[33m\"\u001b[39m: row.get(\u001b[33m\"\u001b[39m\u001b[33mconfirmed_task\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdom_text\u001b[39m\u001b[33m\"\u001b[39m: row.get(\u001b[33m\"\u001b[39m\u001b[33mcleaned_html\u001b[39m\u001b[33m\"\u001b[39m),  \n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maxtree_text\u001b[39m\u001b[33m\"\u001b[39m: row.get(\u001b[33m\"\u001b[39m\u001b[33maxtree\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;66;03m# Check if your version has axtree; otherwise requires conversion\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mscreenshot\u001b[39m\u001b[33m\"\u001b[39m: row.get(\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m),       \u001b[38;5;66;03m# This is already a PIL Image object\u001b[39;00m\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcandidate_elements\u001b[39m\u001b[33m\"\u001b[39m: row.get(\u001b[33m\"\u001b[39m\u001b[33mpos_candidates\u001b[39m\u001b[33m\"\u001b[39m, []) + row.get(\u001b[33m\"\u001b[39m\u001b[33mneg_candidates\u001b[39m\u001b[33m\"\u001b[39m, []),\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlabel_element_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpos_candidates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackend_node_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m row.get(\u001b[33m\"\u001b[39m\u001b[33mpos_candidates\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlabel_action\u001b[39m\u001b[33m\"\u001b[39m: row.get(\u001b[33m\"\u001b[39m\u001b[33moperation\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\u001b[33m\"\u001b[39m\u001b[33mop\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     33\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlabel_value\u001b[39m\u001b[33m\"\u001b[39m: row.get(\u001b[33m\"\u001b[39m\u001b[33moperation\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     34\u001b[39m }\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "class Mind2WebDataset:\n",
    "    \"\"\"\n",
    "    Real loader for osunlp/Multimodal-Mind2Web.\n",
    "    Available splits: 'train', 'test_task', 'test_website', 'test_domain'\n",
    "    \"\"\"\n",
    "    def __init__(self, split=\"test_website\", streaming=True):\n",
    "        # We use streaming=True to pull data on-the-fly from AWS/HF\n",
    "        self.ds = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=split, streaming=streaming)\n",
    "        self.iter_ds = iter(self.ds)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for row in self.ds:\n",
    "            yield self._process_row(row)\n",
    "\n",
    "    def _process_row(self, row):\n",
    "        \"\"\"Maps HuggingFace row schema to your project's expected interface.\"\"\"\n",
    "        # Note: Mind2Web provides 'cleaned_html'. \n",
    "        # For AXTree, you'll need a conversion step (see note below).\n",
    "        \n",
    "        return {\n",
    "            \"action_uid\": row.get(\"action_uid\"),\n",
    "            \"instruction\": row.get(\"confirmed_task\"),\n",
    "            \"dom_text\": row.get(\"cleaned_html\"),  \n",
    "            \"axtree_text\": row.get(\"axtree\", \"\"), # Check if your version has axtree; otherwise requires conversion\n",
    "            \"screenshot\": row.get(\"image\"),       # This is already a PIL Image object\n",
    "            \"candidate_elements\": row.get(\"pos_candidates\", []) + row.get(\"neg_candidates\", []),\n",
    "            \"label_element_id\": row.get(\"pos_candidates\")[0][\"backend_node_id\"] if row.get(\"pos_candidates\") else None,\n",
    "            \"label_action\": row.get(\"operation\", {}).get(\"op\"),\n",
    "            \"label_value\": row.get(\"operation\", {}).get(\"value\"),\n",
    "        }\n",
    "\n",
    "    def get_sample(self):\n",
    "        \"\"\"Helper to get just one sample for testing.\"\"\"\n",
    "        return self._process_row(next(self.iter_ds))\n",
    "\n",
    "# --- Usage in your Notebook ---\n",
    "# Since it's an iterable dataset (streaming), you loop through it or use next()\n",
    "dataset = Mind2WebDataset(split=\"test_website\", streaming=True)\n",
    "sample = dataset.get_sample()\n",
    "\n",
    "print(f\"Task: {sample['instruction']}\")\n",
    "print(f\"Action: {sample['label_action']} on ID {sample['label_element_id']}\")\n",
    "# display(sample['screenshot']) # Use this in Jupyter to see the image\n",
    "\n",
    "#dataset = Mind2WebDataset(split=\"test\")\n",
    "#print(f\"Dataset size: {len(dataset)} samples\")\n",
    "#print(\"Sample keys:\", list(dataset[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4481c74b5f4e7f9bcd363e3c55cb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAW ROW KEYS & TYPES ===\n",
      "  action_uid: str  =  79c4a963-4aa9-49c1-9257-6b0d5069c551\n",
      "  raw_html: str  =  <html backend_node_id=\"113\">\n",
      "  <body backend_node_id=\"188\">\n",
      "    <div backend_node_id=\"189\">\n",
      "      <div backend_node_id=\"\n",
      "  cleaned_html: str  =  <html backend_node_id=\"113\">\n",
      "  <body backend_node_id=\"188\">\n",
      "    <div backend_node_id=\"189\">\n",
      "      <div backend_node_id=\"\n",
      "  operation: str  =  {\"original_op\": \"CLICK\", \"value\": \"\", \"op\": \"CLICK\"}\n",
      "  pos_candidates: list[2]  first item type: <class 'str'>\n",
      "    first item preview: {\"tag\": \"label\", \"attributes\": \"{\\\"backend_node_id\\\": \\\"110\\\", \\\"bounding_box_rect\\\": \\\"356,461,320,34\\\", \\\"class\\\": \\\"b\n",
      "  neg_candidates: list[491]  first item type: <class 'str'>\n",
      "    first item preview: {\"tag\": \"div\", \"attributes\": \"{\\\"backend_node_id\\\": \\\"189\\\", \\\"bounding_box_rect\\\": \\\"0,0,1280,1080\\\", \\\"id\\\": \\\"__next\\\n",
      "  website: str  =  tiktok.music\n",
      "  domain: str  =  Entertainment\n",
      "  subdomain: str  =  Music\n",
      "  annotation_id: str  =  013781df-4391-4533-bcb1-15f6819064f6\n",
      "  confirmed_task: str  =  What are the romantic reggae musics from BCD Studio that can be used in tik tok series in andorra\n",
      "  screenshot: JpegImageFile  =  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x3464 at 0x7F05B1B83B90>\n",
      "  action_reprs: list[7]  first item type: <class 'str'>\n",
      "    first item preview: [label]   -> CLICK\n",
      "  target_action_index: str  =  0\n",
      "  target_action_reprs: str  =  [label]   -> CLICK\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10537e39b1264f6c9df59256194e2533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROCESSED SAMPLE ===\n",
      "Task       : What are the romantic reggae musics from BCD Studio that can be used in tik tok series in andorra\n",
      "Action     : CLICK\n",
      "Label ID   : 110\n",
      "# candidates: 493\n",
      "Screenshot : <class 'NoneType'> \n",
      "DOM length : 85102 chars\n",
      "AXTree     : (empty — not in this dataset version)\n",
      "\n",
      "First 3 candidates:\n",
      "  {'element_id': '110', 'tag': 'label', 'text': ''}\n",
      "  {'element_id': '828', 'tag': 'input', 'text': ''}\n",
      "  {'element_id': '189', 'tag': 'div', 'text': ''}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import io\n",
    "\n",
    "# ── Step 1: Inspect the raw schema first ──────────────────────────────────────\n",
    "# Run this cell alone to see exactly what fields + types HF gives you\n",
    "_ds = load_dataset(\"osunlp/Multimodal-Mind2Web\", split=\"test_website\", streaming=True)\n",
    "_raw = next(iter(_ds))\n",
    "\n",
    "print(\"=== RAW ROW KEYS & TYPES ===\")\n",
    "for k, v in _raw.items():\n",
    "    if isinstance(v, list):\n",
    "        print(f\"  {k}: list[{len(v)}]  first item type: {type(v[0]) if v else 'empty'}\")\n",
    "        if v and isinstance(v[0], str):\n",
    "            print(f\"    first item preview: {v[0][:120]}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {type(v).__name__}  =  {str(v)[:120]}\")\n",
    "\n",
    "# ── Step 2: Fixed dataset class ───────────────────────────────────────────────\n",
    "\n",
    "def _parse_candidate(c):\n",
    "    \"\"\"Candidates may be raw dicts or JSON strings depending on HF version.\"\"\"\n",
    "    if isinstance(c, str):\n",
    "        try:\n",
    "            return json.loads(c)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"backend_node_id\": c, \"tag\": \"\", \"text\": c}\n",
    "    return c  # already a dict\n",
    "\n",
    "\n",
    "def _parse_operation(op):\n",
    "    \"\"\"Operation may be a dict or a JSON string.\"\"\"\n",
    "    if isinstance(op, str):\n",
    "        try:\n",
    "            return json.loads(op)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"op\": op, \"value\": \"\"}\n",
    "    if op is None:\n",
    "        return {\"op\": None, \"value\": \"\"}\n",
    "    return op\n",
    "\n",
    "\n",
    "class Mind2WebDataset:\n",
    "    \"\"\"\n",
    "    Loader for osunlp/Multimodal-Mind2Web.\n",
    "    Splits: 'train', 'test_task', 'test_website', 'test_domain'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split=\"test_website\", streaming=True):\n",
    "        self.ds = load_dataset(\n",
    "            \"osunlp/Multimodal-Mind2Web\", split=split, streaming=streaming\n",
    "        )\n",
    "        self.iter_ds = iter(self.ds)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for row in self.ds:\n",
    "            yield self._process_row(row)\n",
    "\n",
    "    def _process_row(self, row):\n",
    "        # Parse candidates (may be JSON strings or dicts)\n",
    "        pos_raw = row.get(\"pos_candidates\") or []\n",
    "        neg_raw = row.get(\"neg_candidates\") or []\n",
    "        pos_candidates = [_parse_candidate(c) for c in pos_raw]\n",
    "        neg_candidates = [_parse_candidate(c) for c in neg_raw]\n",
    "\n",
    "        # Normalise candidate schema for the rest of the notebook\n",
    "        def normalise(c):\n",
    "            return {\n",
    "                \"element_id\": c.get(\"backend_node_id\", \"\"),\n",
    "                \"tag\":        c.get(\"tag_name\", c.get(\"tag\", \"\")),\n",
    "                \"text\":       c.get(\"text\", \"\"),\n",
    "            }\n",
    "\n",
    "        all_candidates = [normalise(c) for c in pos_candidates + neg_candidates]\n",
    "\n",
    "        # Parse operation\n",
    "        op = _parse_operation(row.get(\"operation\"))\n",
    "\n",
    "        # Label element id\n",
    "        label_element_id = pos_candidates[0].get(\"backend_node_id\") if pos_candidates else None\n",
    "\n",
    "        # Screenshot — HF returns a PIL Image directly for image columns\n",
    "        screenshot = row.get(\"image\")\n",
    "        if isinstance(screenshot, dict) and \"bytes\" in screenshot:\n",
    "            screenshot = Image.open(io.BytesIO(screenshot[\"bytes\"])).convert(\"RGB\")\n",
    "\n",
    "        return {\n",
    "            \"action_uid\":        row.get(\"action_uid\", \"\"),\n",
    "            \"instruction\":       row.get(\"confirmed_task\", \"\"),\n",
    "            \"dom_text\":          row.get(\"cleaned_html\", \"\"),\n",
    "            \"axtree_text\":       row.get(\"axtree\", \"\"),        # empty string if not present\n",
    "            \"screenshot\":        screenshot,\n",
    "            \"candidate_elements\": all_candidates,\n",
    "            \"label_element_id\":  label_element_id,\n",
    "            \"label_action\":      op.get(\"op\"),\n",
    "            \"label_value\":       op.get(\"value\", \"\"),\n",
    "        }\n",
    "\n",
    "    def get_sample(self):\n",
    "        \"\"\"Get one sample for quick testing.\"\"\"\n",
    "        return self._process_row(next(self.iter_ds))\n",
    "\n",
    "\n",
    "# ── Step 3: Test it ───────────────────────────────────────────────────────────\n",
    "dataset = Mind2WebDataset(split=\"test_website\", streaming=True)\n",
    "sample = dataset.get_sample()\n",
    "\n",
    "print(\"\\n=== PROCESSED SAMPLE ===\")\n",
    "print(f\"Task       : {sample['instruction']}\")\n",
    "print(f\"Action     : {sample['label_action']}\")\n",
    "print(f\"Label ID   : {sample['label_element_id']}\")\n",
    "print(f\"# candidates: {len(sample['candidate_elements'])}\")\n",
    "print(f\"Screenshot : {type(sample['screenshot'])} {getattr(sample['screenshot'], 'size', '')}\")\n",
    "print(f\"DOM length : {len(sample['dom_text'])} chars\")\n",
    "print(f\"AXTree     : {sample['axtree_text'][:80] if sample['axtree_text'] else '(empty — not in this dataset version)'}\")\n",
    "print(f\"\\nFirst 3 candidates:\")\n",
    "for c in sample['candidate_elements'][:3]:\n",
    "    print(f\"  {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Templates\n",
    "One template per baseline type. Each returns a string prompt (and optionally an image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates defined.\n"
     ]
    }
   ],
   "source": [
    "def format_candidates(candidate_elements):\n",
    "    \"\"\"Format candidate elements into a numbered list for the prompt.\"\"\"\n",
    "    lines = []\n",
    "    for i, el in enumerate(candidate_elements):\n",
    "        lines.append(f\"[{i}] id={el['element_id']} tag={el['tag']} text=\\\"{el['text']}\\\"\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a web agent. Given a task and webpage information, predict the next action.\\n\"\n",
    "    \"Respond ONLY in JSON format: {\\\"element_id\\\": \\\"<id>\\\", \\\"action\\\": \\\"<click|type|select>\\\", \\\"value\\\": \\\"<optional>\\\"}\\n\"\n",
    "    \"Do not include any explanation outside the JSON.\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_COT = (\n",
    "    \"You are a web agent. Given a task and webpage information, predict the next action.\\n\"\n",
    "    \"First, reason step by step about which element to interact with and why.\\n\"\n",
    "    \"Then respond with a JSON on the final line: {\\\"element_id\\\": \\\"<id>\\\", \\\"action\\\": \\\"<click|type|select>\\\", \\\"value\\\": \\\"<optional>\\\"}\"\n",
    ")\n",
    "\n",
    "MAX_DOM_CHARS = 8000   # ~2000 tokens, leaves room for model weights\n",
    "MAX_AX_CHARS  = 6000\n",
    "\n",
    "def make_prompt(sample, baseline_type, use_cot=False):\n",
    "    \"\"\"\n",
    "    Returns: (text_prompt: str, image: PIL.Image or None)\n",
    "\n",
    "    baseline_type options:\n",
    "        'text_dom'       -> Baseline 1: DOM text only\n",
    "        'image_only'     -> Baseline 2: Image only\n",
    "        'multimodal_dom' -> Baselines 3/4: Image + DOM\n",
    "        'axtree_only'    -> Baseline 5: AXTree text only\n",
    "        'multimodal_ax'  -> Baselines 6/7: Image + AXTree\n",
    "    \"\"\"\n",
    "    instruction = sample[\"instruction\"]\n",
    "    candidates  = format_candidates(sample[\"candidate_elements\"])\n",
    "    dom_text    = sample[\"dom_text\"][:MAX_DOM_CHARS] \n",
    "    axtree_text = sample[\"axtree_text\"][:MAX_AX_CHARS] \n",
    "    image       = sample[\"screenshot\"]\n",
    "\n",
    "    sys = SYSTEM_PROMPT_COT if use_cot else SYSTEM_PROMPT\n",
    "\n",
    "    if baseline_type == \"text_dom\":\n",
    "        prompt = (\n",
    "            f\"{sys}\\n\\n\"\n",
    "            f\"Task: {instruction}\\n\\n\"\n",
    "            f\"DOM:\\n{dom_text}\\n\\n\"\n",
    "            f\"Candidate elements:\\n{candidates}\\n\\n\"\n",
    "            f\"Next action:\"\n",
    "        )\n",
    "        return prompt, None\n",
    "\n",
    "    elif baseline_type == \"image_only\":\n",
    "        prompt = (\n",
    "            f\"{sys}\\n\\n\"\n",
    "            f\"Task: {instruction}\\n\\n\"\n",
    "            f\"Candidate elements:\\n{candidates}\\n\\n\"\n",
    "            \"The screenshot of the current webpage is provided.\\n\"\n",
    "            f\"Next action:\"\n",
    "        )\n",
    "        return prompt, image\n",
    "\n",
    "    elif baseline_type == \"multimodal_dom\":\n",
    "        prompt = (\n",
    "            f\"{sys}\\n\\n\"\n",
    "            f\"Task: {instruction}\\n\\n\"\n",
    "            f\"DOM:\\n{dom_text}\\n\\n\"\n",
    "            f\"Candidate elements:\\n{candidates}\\n\\n\"\n",
    "            \"The screenshot of the current webpage is provided.\\n\"\n",
    "            f\"Next action:\"\n",
    "        )\n",
    "        return prompt, image\n",
    "\n",
    "    elif baseline_type == \"axtree_only\":\n",
    "        prompt = (\n",
    "            f\"{sys}\\n\\n\"\n",
    "            f\"Task: {instruction}\\n\\n\"\n",
    "            f\"Accessibility Tree:\\n{axtree_text}\\n\\n\"\n",
    "            f\"Candidate elements:\\n{candidates}\\n\\n\"\n",
    "            f\"Next action:\"\n",
    "        )\n",
    "        return prompt, None\n",
    "\n",
    "    elif baseline_type == \"multimodal_ax\":\n",
    "        prompt = (\n",
    "            f\"{sys}\\n\\n\"\n",
    "            f\"Task: {instruction}\\n\\n\"\n",
    "            f\"Accessibility Tree:\\n{axtree_text}\\n\\n\"\n",
    "            f\"Candidate elements:\\n{candidates}\\n\\n\"\n",
    "            \"The screenshot of the current webpage is provided.\\n\"\n",
    "            f\"Next action:\"\n",
    "        )\n",
    "        return prompt, image\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown baseline_type: {baseline_type}\")\n",
    "\n",
    "\n",
    "print(\"Prompt templates defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Wrappers\n",
    "Unified `predict()` interface for both Qwen2-VL and InternVL2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model wrappers defined. Run the next cell to load a model.\n"
     ]
    }
   ],
   "source": [
    "class Qwen2VLWrapper:\n",
    "    \"\"\"Wrapper for Qwen2-VL-7B-Instruct\"\"\"\n",
    "\n",
    "    def __init__(self, model_id=CONFIG[\"model_a\"]):\n",
    "        print(f\"Loading Qwen2-VL from {model_id}...\")\n",
    "        from qwen_vl_utils import process_vision_info\n",
    "        self.process_vision_info = process_vision_info\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=CONFIG[\"dtype\"],\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.model.eval()\n",
    "        print(\"Qwen2-VL loaded.\")\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, text_prompt, image=None, max_new_tokens=CONFIG[\"max_new_tokens\"]):\n",
    "        messages = []\n",
    "        content = []\n",
    "        if image is not None:\n",
    "            content.append({\"type\": \"image\", \"image\": image})\n",
    "        content.append({\"type\": \"text\", \"text\": text_prompt})\n",
    "        messages.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "        text = self.processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        image_inputs, video_inputs = self.process_vision_info(messages)\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(CONFIG[\"device\"])\n",
    "\n",
    "        generated = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        out = self.processor.decode(\n",
    "            generated[0][inputs.input_ids.shape[1]:], skip_special_tokens=True\n",
    "        )\n",
    "        return out.strip()\n",
    "\n",
    "\n",
    "# class InternVL2Wrapper:\n",
    "#     \"\"\"Wrapper for InternVL2-8B\"\"\"\n",
    "\n",
    "#     def __init__(self, model_id=CONFIG[\"model_b\"]):\n",
    "#         print(f\"Loading InternVL2 from {model_id}...\")\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "#         self.model = AutoModel.from_pretrained(\n",
    "#             model_id,\n",
    "#             torch_dtype=CONFIG[\"dtype\"],\n",
    "#             device_map=\"auto\",\n",
    "#             trust_remote_code=True,\n",
    "#         )\n",
    "#         self.model.eval()\n",
    "#         print(\"InternVL2 loaded.\")\n",
    "\n",
    "#     def _load_image(self, image, max_num=6):\n",
    "#         \"\"\"Convert PIL image to InternVL2 pixel_values format.\"\"\"\n",
    "#         import torchvision.transforms as T\n",
    "#         from torchvision.transforms.functional import InterpolationMode\n",
    "#         IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "#         IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "#         transform = T.Compose([\n",
    "#             T.Resize((448, 448), interpolation=InterpolationMode.BICUBIC),\n",
    "#             T.ToTensor(),\n",
    "#             T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "#         ])\n",
    "#         pixel_values = transform(image.convert(\"RGB\")).unsqueeze(0)\n",
    "#         return pixel_values.to(CONFIG[\"dtype\"]).to(CONFIG[\"device\"])\n",
    "\n",
    "#     @torch.inference_mode()\n",
    "#     def predict(self, text_prompt, image=None, max_new_tokens=CONFIG[\"max_new_tokens\"]):\n",
    "#         gen_config = {\"max_new_tokens\": max_new_tokens, \"do_sample\": False}\n",
    "\n",
    "#         if image is not None:\n",
    "#             pixel_values = self._load_image(image)\n",
    "#             prompt = f\"<image>\\n{text_prompt}\"\n",
    "#             response = self.model.chat(\n",
    "#                 self.tokenizer, pixel_values, prompt, gen_config\n",
    "#             )\n",
    "#         else:\n",
    "#             pixel_values = None\n",
    "#             response = self.model.chat(\n",
    "#                 self.tokenizer, pixel_values, text_prompt, gen_config\n",
    "#             )\n",
    "#         return response.strip()\n",
    "\n",
    "\n",
    "print(\"Model wrappers defined. Run the next cell to load a model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output Parsing & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_prediction(raw_output):\n",
    "    \"\"\"\n",
    "    Extract JSON from model output.\n",
    "    Returns dict with keys: element_id, action, value\n",
    "    Returns None on parse failure.\n",
    "    \"\"\"\n",
    "    # Try to find JSON in the output (handles CoT where JSON is at the end)\n",
    "    json_pattern = r'\\{[^{}]*\\}'\n",
    "    matches = re.findall(json_pattern, raw_output, re.DOTALL)\n",
    "\n",
    "    for match in reversed(matches):  # take last JSON block (CoT reasoning first, JSON last)\n",
    "        try:\n",
    "            parsed = json.loads(match)\n",
    "            if \"element_id\" in parsed and \"action\" in parsed:\n",
    "                parsed.setdefault(\"value\", \"\")\n",
    "                return parsed\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def evaluate_prediction(pred, sample):\n",
    "    \"\"\"\n",
    "    Returns dict of per-sample metrics.\n",
    "    \"\"\"\n",
    "    if pred is None:\n",
    "        return {\"element_acc\": 0, \"action_acc\": 0, \"exact_match\": 0, \"parse_fail\": 1}\n",
    "\n",
    "    element_correct = (str(pred[\"element_id\"]) == str(sample[\"label_element_id\"]))\n",
    "    action_correct  = (pred[\"action\"].lower() == sample[\"label_action\"].lower())\n",
    "    exact_match     = element_correct and action_correct\n",
    "\n",
    "    return {\n",
    "        \"element_acc\": int(element_correct),\n",
    "        \"action_acc\":  int(action_correct),\n",
    "        \"exact_match\": int(exact_match),\n",
    "        \"parse_fail\":  0,\n",
    "    }\n",
    "\n",
    "\n",
    "def aggregate_metrics(results):\n",
    "    keys = [\"element_acc\", \"action_acc\", \"exact_match\", \"parse_fail\"]\n",
    "    agg = {k: np.mean([r[k] for r in results]) for k in keys}\n",
    "    agg[\"n\"] = len(results)\n",
    "    return agg\n",
    "\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Inference Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_baseline() defined.\n"
     ]
    }
   ],
   "source": [
    "# def run_baseline(\n",
    "#     model_wrapper,\n",
    "#     dataset,\n",
    "#     baseline_type,\n",
    "#     baseline_name,\n",
    "#     use_cot=False,\n",
    "#     max_samples=CONFIG[\"max_samples\"],\n",
    "#     results_dir=CONFIG[\"results_dir\"],\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Run inference for one baseline and save results.\n",
    "\n",
    "#     Args:\n",
    "#         model_wrapper : Qwen2VLWrapper or InternVL2Wrapper instance\n",
    "#         dataset       : Mind2WebDataset\n",
    "#         baseline_type : str — one of 'text_dom', 'image_only', 'multimodal_dom',\n",
    "#                                'axtree_only', 'multimodal_ax'\n",
    "#         baseline_name : str — used for output file naming (e.g. 'B1_text_dom_qwen')\n",
    "#         use_cot       : bool — whether to use chain-of-thought prompt\n",
    "#         max_samples   : int or None — limit for debugging\n",
    "#     \"\"\"\n",
    "#     max_new_tokens = 512 if use_cot else CONFIG[\"max_new_tokens\"]\n",
    "#     n = min(len(dataset), max_samples) if max_samples else len(dataset)\n",
    "\n",
    "#     all_results = []\n",
    "#     raw_outputs = []\n",
    "\n",
    "#     for i in tqdm(range(n), desc=baseline_name):\n",
    "#         sample = dataset[i]\n",
    "#         prompt, image = make_prompt(sample, baseline_type, use_cot=use_cot)\n",
    "\n",
    "#         try:\n",
    "#             raw = model_wrapper.predict(prompt, image=image, max_new_tokens=max_new_tokens)\n",
    "#         except Exception as e:\n",
    "#             print(f\"[{i}] Inference error: {e}\")\n",
    "#             raw = \"\"\n",
    "\n",
    "#         pred    = parse_prediction(raw)\n",
    "#         metrics = evaluate_prediction(pred, sample)\n",
    "\n",
    "#         all_results.append(metrics)\n",
    "#         raw_outputs.append({\n",
    "#             \"task_id\":   sample[\"task_id\"],\n",
    "#             \"raw\":       raw,\n",
    "#             \"parsed\":    pred,\n",
    "#             \"gt_element\": sample[\"label_element_id\"],\n",
    "#             \"gt_action\":  sample[\"label_action\"],\n",
    "#             **metrics,\n",
    "#         })\n",
    "\n",
    "#     # Save raw outputs\n",
    "#     out_path = Path(results_dir) / f\"{baseline_name}_raw.jsonl\"\n",
    "#     with open(out_path, \"w\") as f:\n",
    "#         for row in raw_outputs:\n",
    "#             f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "#     # Aggregate and save summary\n",
    "#     summary = aggregate_metrics(all_results)\n",
    "#     summary_path = Path(results_dir) / f\"{baseline_name}_summary.json\"\n",
    "#     with open(summary_path, \"w\") as f:\n",
    "#         json.dump({\"baseline\": baseline_name, **summary}, f, indent=2)\n",
    "\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"Baseline: {baseline_name}\")\n",
    "#     print(f\"  Element Acc : {summary['element_acc']:.3f}\")\n",
    "#     print(f\"  Action Acc  : {summary['action_acc']:.3f}\")\n",
    "#     print(f\"  Exact Match : {summary['exact_match']:.3f}\")\n",
    "#     print(f\"  Parse Fail  : {summary['parse_fail']:.3f}\")\n",
    "#     print(f\"  N samples   : {summary['n']}\")\n",
    "#     print(f\"{'='*50}\\n\")\n",
    "\n",
    "#     return summary\n",
    "\n",
    "\n",
    "# print(\"run_baseline() defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(\n",
    "    model_wrapper,\n",
    "    dataset,\n",
    "    baseline_type,\n",
    "    baseline_name,\n",
    "    use_cot=False,\n",
    "    max_samples=CONFIG[\"max_samples\"],\n",
    "    results_dir=CONFIG[\"results_dir\"],\n",
    "):\n",
    "    max_new_tokens = 512 if use_cot else CONFIG[\"max_new_tokens\"]\n",
    "\n",
    "    all_results = []\n",
    "    raw_outputs = []\n",
    "\n",
    "    # Streaming-compatible loop — use max_samples to cap, or run until exhausted\n",
    "    dataset_iter = iter(dataset)\n",
    "    pbar = tqdm(desc=baseline_name, total=max_samples)\n",
    "\n",
    "    for i, sample in enumerate(dataset_iter):\n",
    "        if max_samples and i >= max_samples:\n",
    "            break\n",
    "\n",
    "        prompt, image = make_prompt(sample, baseline_type, use_cot=use_cot)\n",
    "\n",
    "        try:\n",
    "            raw = model_wrapper.predict(prompt, image=image, max_new_tokens=max_new_tokens)\n",
    "        except Exception as e:\n",
    "            print(f\"[{i}] Inference error: {e}\")\n",
    "            raw = \"\"\n",
    "        finally:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        pred    = parse_prediction(raw)\n",
    "        metrics = evaluate_prediction(pred, sample)\n",
    "\n",
    "        all_results.append(metrics)\n",
    "        raw_outputs.append({\n",
    "            \"task_id\":    sample.get(\"action_uid\", str(i)),\n",
    "            \"raw\":        raw,\n",
    "            \"parsed\":     pred,\n",
    "            \"gt_element\": sample[\"label_element_id\"],\n",
    "            \"gt_action\":  sample[\"label_action\"],\n",
    "            **metrics,\n",
    "        })\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Save raw outputs\n",
    "    out_path = Path(results_dir) / f\"{baseline_name}_raw.jsonl\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        for row in raw_outputs:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "    # Aggregate and save summary\n",
    "    summary = aggregate_metrics(all_results)\n",
    "    summary_path = Path(results_dir) / f\"{baseline_name}_summary.json\"\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        json.dump({\"baseline\": baseline_name, **summary}, f, indent=2)\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Baseline: {baseline_name}\")\n",
    "    print(f\"  Element Acc : {summary['element_acc']:.3f}\")\n",
    "    print(f\"  Action Acc  : {summary['action_acc']:.3f}\")\n",
    "    print(f\"  Exact Match : {summary['exact_match']:.3f}\")\n",
    "    print(f\"  Parse Fail  : {summary['parse_fail']:.3f}\")\n",
    "    print(f\"  N samples   : {summary['n']}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run All Baselines\n",
    "\n",
    "The 10 baselines map as follows:\n",
    "\n",
    "| # | Description | baseline_type | use_cot | model |\n",
    "|---|---|---|---|---|\n",
    "| 1 | Text DOM only | `text_dom` | False | either |\n",
    "| 2 | Image only | `image_only` | False | either |\n",
    "| 3 | Multimodal - Model A | `multimodal_dom` | False | Qwen2-VL |\n",
    "| 4 | Multimodal - Model B | `multimodal_dom` | False | InternVL2 |\n",
    "| 5 | AXTree only | `axtree_only` | False | either |\n",
    "| 6 | Multimodal + AXTree - Model A | `multimodal_ax` | False | Qwen2-VL |\n",
    "| 7 | Multimodal + AXTree - Model B | `multimodal_ax` | False | InternVL2 |\n",
    "| 8 | 3 + CoT | `multimodal_dom` | True | Qwen2-VL |\n",
    "| 9 | 5 + CoT - Model A | `multimodal_ax` | True | Qwen2-VL |\n",
    "| 10 | 5 + CoT - Model B | `multimodal_ax` | True | InternVL2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen2-VL from Qwen/Qwen2-VL-7B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59a4b66572f4fbd9b53086b71e98957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dde1b40a0114ad2a0830aa641a5ffd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59553ed713864868821785fce37fa75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2-VL loaded.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 1: Load Model A (Qwen2-VL)\n",
    "# Run this cell, then run Qwen baselines below.\n",
    "# Then unload and load Model B to save VRAM.\n",
    "# ============================================================\n",
    "model_a = Qwen2VLWrapper()\n",
    "all_summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Qwen2VLWrapper at 0x7f05b1bd2300>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B1_text_dom_qwen:  24%|███████████████▎                                                | 24/100 [01:22<05:16,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24] Inference error: CUDA out of memory. Tried to allocate 1.21 GiB. GPU 0 has a total capacity of 22.06 GiB of which 1.14 GiB is free. Including non-PyTorch memory, this process has 20.91 GiB memory in use. Of the allocated memory 18.95 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B1_text_dom_qwen:  30%|███████████████████▏                                            | 30/100 [01:42<03:30,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29] Inference error: CUDA out of memory. Tried to allocate 1.58 GiB. GPU 0 has a total capacity of 22.06 GiB of which 411.44 MiB is free. Including non-PyTorch memory, this process has 21.65 GiB memory in use. Of the allocated memory 19.94 GiB is allocated by PyTorch, and 1.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "B1_text_dom_qwen:  31%|███████████████████▊                                            | 31/100 [01:42<02:40,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30] Inference error: CUDA out of memory. Tried to allocate 1.61 GiB. GPU 0 has a total capacity of 22.06 GiB of which 351.44 MiB is free. Including non-PyTorch memory, this process has 21.71 GiB memory in use. Of the allocated memory 20.02 GiB is allocated by PyTorch, and 1.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "B1_text_dom_qwen:  32%|████████████████████▍                                           | 32/100 [01:43<02:04,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31] Inference error: CUDA out of memory. Tried to allocate 1.61 GiB. GPU 0 has a total capacity of 22.06 GiB of which 335.44 MiB is free. Including non-PyTorch memory, this process has 21.72 GiB memory in use. Of the allocated memory 20.04 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B1_text_dom_qwen:  34%|█████████████████████▊                                          | 34/100 [01:53<03:46,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34] Inference error: CUDA out of memory. Tried to allocate 1.06 GiB. GPU 0 has a total capacity of 22.06 GiB of which 369.44 MiB is free. Including non-PyTorch memory, this process has 21.69 GiB memory in use. Of the allocated memory 19.42 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B1_text_dom_qwen:  37%|███████████████████████▋                                        | 37/100 [02:03<03:10,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36] Inference error: CUDA out of memory. Tried to allocate 1.51 GiB. GPU 0 has a total capacity of 22.06 GiB of which 543.44 MiB is free. Including non-PyTorch memory, this process has 21.52 GiB memory in use. Of the allocated memory 19.77 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B1_text_dom_qwen:  66%|██████████████████████████████████████████▏                     | 66/100 [03:57<02:07,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66] Inference error: CUDA out of memory. Tried to allocate 1.11 GiB. GPU 0 has a total capacity of 22.06 GiB of which 219.44 MiB is free. Including non-PyTorch memory, this process has 21.84 GiB memory in use. Of the allocated memory 19.22 GiB is allocated by PyTorch, and 2.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B1_text_dom_qwen:  71%|█████████████████████████████████████████████▍                  | 71/100 [04:14<01:24,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70] Inference error: CUDA out of memory. Tried to allocate 1.55 GiB. GPU 0 has a total capacity of 22.06 GiB of which 467.44 MiB is free. Including non-PyTorch memory, this process has 21.59 GiB memory in use. Of the allocated memory 19.87 GiB is allocated by PyTorch, and 1.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "B1_text_dom_qwen:  72%|██████████████████████████████████████████████                  | 72/100 [04:14<01:02,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71] Inference error: CUDA out of memory. Tried to allocate 1.52 GiB. GPU 0 has a total capacity of 22.06 GiB of which 535.44 MiB is free. Including non-PyTorch memory, this process has 21.53 GiB memory in use. Of the allocated memory 19.78 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B1_text_dom_qwen:  77%|█████████████████████████████████████████████████▎              | 77/100 [04:36<01:16,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77] Inference error: CUDA out of memory. Tried to allocate 1.40 GiB. GPU 0 has a total capacity of 22.06 GiB of which 767.44 MiB is free. Including non-PyTorch memory, this process has 21.30 GiB memory in use. Of the allocated memory 19.48 GiB is allocated by PyTorch, and 1.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "B1_text_dom_qwen:  78%|█████████████████████████████████████████████████▉              | 78/100 [04:37<00:54,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78] Inference error: CUDA out of memory. Tried to allocate 1.46 GiB. GPU 0 has a total capacity of 22.06 GiB of which 655.44 MiB is free. Including non-PyTorch memory, this process has 21.41 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B1_text_dom_qwen:  84%|█████████████████████████████████████████████████████▊          | 84/100 [04:53<00:39,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83] Inference error: CUDA out of memory. Tried to allocate 1.67 GiB. GPU 0 has a total capacity of 22.06 GiB of which 215.44 MiB is free. Including non-PyTorch memory, this process has 21.84 GiB memory in use. Of the allocated memory 20.20 GiB is allocated by PyTorch, and 1.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "B1_text_dom_qwen:  85%|██████████████████████████████████████████████████████▍         | 85/100 [04:53<00:29,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84] Inference error: CUDA out of memory. Tried to allocate 1.73 GiB. GPU 0 has a total capacity of 22.06 GiB of which 99.44 MiB is free. Including non-PyTorch memory, this process has 21.95 GiB memory in use. Of the allocated memory 20.35 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "B1_text_dom_qwen:  86%|███████████████████████████████████████████████████████         | 86/100 [04:54<00:22,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85] Inference error: CUDA out of memory. Tried to allocate 1.66 GiB. GPU 0 has a total capacity of 22.06 GiB of which 239.44 MiB is free. Including non-PyTorch memory, this process has 21.82 GiB memory in use. Of the allocated memory 20.17 GiB is allocated by PyTorch, and 1.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[86] Inference error: CUDA out of memory. Tried to allocate 1.37 GiB. GPU 0 has a total capacity of 22.06 GiB of which 835.44 MiB is free. Including non-PyTorch memory, this process has 21.23 GiB memory in use. Of the allocated memory 19.39 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B1_text_dom_qwen:  90%|█████████████████████████████████████████████████████████▌      | 90/100 [05:12<00:47,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90] Inference error: CUDA out of memory. Tried to allocate 930.00 MiB. GPU 0 has a total capacity of 22.06 GiB of which 849.44 MiB is free. Including non-PyTorch memory, this process has 21.22 GiB memory in use. Of the allocated memory 19.17 GiB is allocated by PyTorch, and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B1_text_dom_qwen: 100%|███████████████████████████████████████████████████████████████| 100/100 [06:06<00:00,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Baseline: B1_text_dom_qwen\n",
      "  Element Acc : 0.090\n",
      "  Action Acc  : 0.520\n",
      "  Exact Match : 0.090\n",
      "  Parse Fail  : 0.160\n",
      "  N samples   : 100\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline 1: Text DOM only (model-agnostic, run with model_a)\n",
    "all_summaries[\"B1_text_dom\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"text_dom\",\n",
    "    baseline_name=\"B1_text_dom_qwen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "summary_path = Path(CONFIG[\"results_dir\"]) / \"B1_text_dom_qwen_summary.json\"\n",
    "with open(summary_path) as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(f\"Baseline : {summary['baseline']}\")\n",
    "print(f\"N samples: {summary['n']}\")\n",
    "print(f\"Element Acc : {summary['element_acc']:.3f}  ({int(summary['element_acc']*summary['n'])}/{int(summary['n'])})\")\n",
    "print(f\"Action Acc  : {summary['action_acc']:.3f}  ({int(summary['action_acc']*summary['n'])}/{int(summary['n'])})\")\n",
    "print(f\"Exact Match : {summary['exact_match']:.3f}  ({int(summary['exact_match']*summary['n'])}/{int(summary['n'])})\")\n",
    "print(f\"Parse Fail  : {summary['parse_fail']:.3f}  ({int(summary['parse_fail']*summary['n'])}/{int(summary['n'])})\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 2: Image only\n",
    "all_summaries[\"B2_image_only\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"image_only\",\n",
    "    baseline_name=\"B2_image_only_qwen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 3: Multimodal DOM - Model A (Qwen2-VL)\n",
    "all_summaries[\"B3_multimodal_dom_qwen\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"multimodal_dom\",\n",
    "    baseline_name=\"B3_multimodal_dom_qwen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 5: AXTree only\n",
    "all_summaries[\"B5_axtree_only\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"axtree_only\",\n",
    "    baseline_name=\"B5_axtree_only_qwen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 6: Multimodal AXTree - Model A (Qwen2-VL)\n",
    "all_summaries[\"B6_multimodal_ax_qwen\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"multimodal_ax\",\n",
    "    baseline_name=\"B6_multimodal_ax_qwen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 8: Multimodal DOM + CoT (Model A)\n",
    "all_summaries[\"B8_multimodal_dom_cot_qwen\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"multimodal_dom\",\n",
    "    baseline_name=\"B8_multimodal_dom_cot_qwen\",\n",
    "    use_cot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 9: AXTree + CoT - Model A (Qwen2-VL)\n",
    "all_summaries[\"B9_axtree_cot_qwen\"] = run_baseline(\n",
    "    model_a, dataset,\n",
    "    baseline_type=\"multimodal_ax\",\n",
    "    baseline_name=\"B9_axtree_cot_qwen\",\n",
    "    use_cot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload Model A to free VRAM before loading Model B\n",
    "del model_a\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model A unloaded. VRAM freed.\")\n",
    "print(f\"VRAM used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: Load Model B (InternVL2)\n",
    "# ============================================================\n",
    "model_b = InternVL2Wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 4: Multimodal DOM - Model B (InternVL2)\n",
    "all_summaries[\"B4_multimodal_dom_internvl\"] = run_baseline(\n",
    "    model_b, dataset,\n",
    "    baseline_type=\"multimodal_dom\",\n",
    "    baseline_name=\"B4_multimodal_dom_internvl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 7: Multimodal AXTree - Model B (InternVL2)\n",
    "all_summaries[\"B7_multimodal_ax_internvl\"] = run_baseline(\n",
    "    model_b, dataset,\n",
    "    baseline_type=\"multimodal_ax\",\n",
    "    baseline_name=\"B7_multimodal_ax_internvl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 10: AXTree + CoT - Model B (InternVL2)\n",
    "all_summaries[\"B10_axtree_cot_internvl\"] = run_baseline(\n",
    "    model_b, dataset,\n",
    "    baseline_type=\"multimodal_ax\",\n",
    "    baseline_name=\"B10_axtree_cot_internvl\",\n",
    "    use_cot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "baseline_labels = {\n",
    "    \"B1_text_dom\":               \"B1: Text DOM only\",\n",
    "    \"B2_image_only\":             \"B2: Image only\",\n",
    "    \"B3_multimodal_dom_qwen\":    \"B3: Multimodal DOM (Qwen2-VL)\",\n",
    "    \"B4_multimodal_dom_internvl\":\"B4: Multimodal DOM (InternVL2)\",\n",
    "    \"B5_axtree_only\":            \"B5: AXTree only\",\n",
    "    \"B6_multimodal_ax_qwen\":     \"B6: Multimodal AXTree (Qwen2-VL)\",\n",
    "    \"B7_multimodal_ax_internvl\": \"B7: Multimodal AXTree (InternVL2)\",\n",
    "    \"B8_multimodal_dom_cot_qwen\":\"B8: Multimodal DOM + CoT (Qwen2-VL)\",\n",
    "    \"B9_axtree_cot_qwen\":        \"B9: AXTree + CoT (Qwen2-VL)\",\n",
    "    \"B10_axtree_cot_internvl\":   \"B10: AXTree + CoT (InternVL2)\",\n",
    "}\n",
    "\n",
    "for key, label in baseline_labels.items():\n",
    "    if key in all_summaries:\n",
    "        s = all_summaries[key]\n",
    "        rows.append({\n",
    "            \"Baseline\": label,\n",
    "            \"Element Acc\": f\"{s['element_acc']:.3f}\",\n",
    "            \"Action Acc\":  f\"{s['action_acc']:.3f}\",\n",
    "            \"Exact Match\": f\"{s['exact_match']:.3f}\",\n",
    "            \"Parse Fail\":  f\"{s['parse_fail']:.3f}\",\n",
    "            \"N\": int(s['n']),\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(Path(CONFIG[\"results_dir\"]) / \"all_baselines_summary.csv\", index=False)\n",
    "print(\"\\nSaved to all_baselines_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "x = range(len(df))\n",
    "ax.bar(x, df[\"Exact Match\"].astype(float), color=\"steelblue\", alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"B{i+1}\" for i in x], fontsize=9)\n",
    "ax.set_ylabel(\"Exact Match Accuracy\")\n",
    "ax.set_title(\"Mind2Web Baselines — Exact Match Accuracy\")\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    ax.text(i, float(row[\"Exact Match\"]) + 0.01, row[\"Exact Match\"], ha=\"center\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(CONFIG[\"results_dir\"]) / \"baseline_results.png\", dpi=150)\n",
    "plt.show()\n",
    "print(\"Plot saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Debugging Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a single sample and model output\n",
    "sample = dataset[0]\n",
    "print(\"Instruction:\", sample[\"instruction\"])\n",
    "print(\"Label element:\", sample[\"label_element_id\"])\n",
    "print(\"Label action:\", sample[\"label_action\"])\n",
    "\n",
    "prompt, image = make_prompt(sample, \"multimodal_ax\", use_cot=False)\n",
    "print(\"\\n--- Prompt (first 800 chars) ---\")\n",
    "print(prompt[:800])\n",
    "\n",
    "if image:\n",
    "    display(image.resize((400, 225)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect a saved results file\n",
    "results_file = Path(CONFIG[\"results_dir\"]) / \"B6_multimodal_ax_qwen_raw.jsonl\"\n",
    "if results_file.exists():\n",
    "    with open(results_file) as f:\n",
    "        lines = [json.loads(l) for l in f]\n",
    "    \n",
    "    failures = [l for l in lines if l[\"element_acc\"] == 0]\n",
    "    print(f\"Total: {len(lines)}, Failures: {len(failures)}\")\n",
    "    print(\"\\nExample failure:\")\n",
    "    print(json.dumps(failures[0], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
